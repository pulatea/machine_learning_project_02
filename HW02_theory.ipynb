{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71e5a95",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681ded3",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "\n",
    "* For every separate problem you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "\n",
    "\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but *discouraged*.\n",
    "\n",
    "## $\\LaTeX$ in Jupyter\n",
    "\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "\n",
    "* to write **cases of equations** use \n",
    "```markdown\n",
    "$$ left-hand-side = \\begin{cases}\n",
    "                     right-hand-side on line 1, & \\text{condition} \\\\\n",
    "                     right-hand-side on line 2, & \\text{condition} \\\\\n",
    "                    \\end{cases} $$\n",
    "```\n",
    "\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "$$ \\begin{align}\n",
    "    left-hand-side on line 1 &= right-hand-side on line 1 \\\\\n",
    "    left-hand-side on line 2 &= right-hand-side on line 2\n",
    "   \\end{align} $$\n",
    "```\n",
    "\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cee62",
   "metadata": {},
   "source": [
    "## Task 1: Kernel theory [8 points]\n",
    "\n",
    "Let $K(x, x'): \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R}$ be a kernel, and $\\phi: \\mathbb{R}^n \\to \\mathbb{R}^m$ its **unknown** feature mapping. For $x, x' \\in \\mathbb{R}^n$ derive the squared Euclidean distance between $\\phi(x)$ and $\\phi(x')$ only in terms of $K(\\cdot, \\cdot)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e07a2f",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15374dff",
   "metadata": {},
   "source": [
    "If the Euclidean distance between two vectors is defined as $d = \\|\\phi(x) - \\phi(x')\\|$, then the squared Euclidean is given by $d^2 = \\|\\phi(x) - \\phi(x')\\|^2$. \n",
    "\n",
    "Utilizing the definition $|V|^2 = V^T V$, then the aforementioned expression gets expanded to:\n",
    "\n",
    "$d^2 = \\|\\phi(x) - \\phi(x')\\|^2  =  (\\phi(x)-\\phi(x'))^T  (\\phi(x)-\\phi(x'))\n",
    " = \\phi(x)^T \\phi(x) - \\phi(x)^T \\phi(x') - \\phi(x')^T \\phi(x) + \\phi(x')^T \\phi(x')$\n",
    " \n",
    "The result expression can be expressed in terms of the Kernel function:\n",
    "\n",
    "$\\|\\phi(x) - \\phi(x')\\|^2 = K(x, x) - K(x, x') - K(x', x) + K(x', x')$.\n",
    "\n",
    "Considering Kernel's symmetry property \\K(x, x') = K(x', x)\\, the final expression gets simplified to:\n",
    "\n",
    "$\\|\\phi(x) - \\phi(x')\\|^2 = K(x, x) - 2K(x, x') + K(x', x')$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f6720",
   "metadata": {},
   "source": [
    "## Task 2: SVM [9 points]\n",
    "\n",
    "Show that for a two-class SVM classifier trained on a linearly separable dataset $(x_i, y_i)_{i =1}^n$ the following upper bound on the leave-one-out-cross-validation error holds true:\n",
    "\n",
    "$$\n",
    "L_1OCV = \\frac{1}{n} \\sum_{i = 1}^n \\delta(y_i \\ne f_i(x_i)) \\le \\frac{|SV|}{n},\n",
    "$$\n",
    "\n",
    "where $\\delta(c) = 1$ if $c$ is True and $\\delta(c) = 0$ if $c$ is False;  \n",
    "for all $i = 1, \\dots, n$ $f_i(x_i)$ is the SVM classifier fitted on the entire data without the observation $(x_i, y_i)$ and $|SV|$ is the number of support vectors of the SVM classifier fitted on the entire data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3f490",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff523e",
   "metadata": {},
   "source": [
    "We consider the necessary components as follows:\n",
    "\n",
    "$X$ = entire data\n",
    "\n",
    "$SVM(X)$ = SVM classifier fitted on the entire data\n",
    "\n",
    "$SVM(X_i)$ = SVM classifier fitted on the entire data, without the observation i: $(x_i, y_i)$\n",
    "\n",
    "$SV$ = set of Support vectors for $SVM(X)$.\n",
    "\n",
    "The $L_1OCV$ definition is $L_1OCV = \\frac{1}{n} \\sum_{i = 1}^n \\delta(y_i \\ne SVM(X_i)$.\n",
    "\n",
    "If the two-class SVM classifier separates the linearly separable data into two classes, we consider the case where $y_i \\ne SVM(X_i)$. This case tells that the point for the observation $(x_i, y_i)$ is on the wrong side of the decision boundary $f_i(x)$. If the observation $(x_i, y_i)$ is in the opposite class, then the right classification for it is being a support vector for $SVM(X)$, knowing that the point lies within or on the margin. \n",
    "\n",
    "We plug this information in the aforementioned $L_1OCV$ definition, and we get:\n",
    "$L_1OCV = \\frac{1}{n} \\sum_{i = 1}^n \\delta(y_i \\ne SVM(X_i)) \\le \\frac{1}{n} \\sum_{i = 1}^n \\delta(x_iy_i)$.\n",
    "Because each $(x_iy_i)$ is a misclassified point that belongs to a Support Vector for $f(x)$, the sum function is counting the amount of all Support Vectors, so the given upper bound $L_1OCV = \\frac{1}{n} \\sum_{i = 1}^n \\delta(y_i \\ne f_i(x_i)) \\le \\frac{|SV|}{n}$ holds true. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb203f",
   "metadata": {},
   "source": [
    "## Task 3. Decision Tree Leaves [6 points]\n",
    "\n",
    "Consider a leaf of a decision tree that consists of object-label pairs $(x_{1}, y_{1}), \\dots, (x_{n}, y_{n})$.\n",
    "\n",
    "The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefc354",
   "metadata": {},
   "source": [
    "Find the **optimal prediction** in the leaf, for a regression tree, i.e. $y_{i} \\in \\mathbb{R}$, and squared percentage error loss $\\mathcal{L}(y, \\hat{y}) = \\cfrac{\\left(y - \\hat{y} \\right)^{2}}{y^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa5b6e",
   "metadata": {},
   "source": [
    "### Your solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbad7a3622198c1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In order to find the optimal prediction in the leaf, the squared error loss given by $\\mathcal{L}(y, \\hat{y}) = \\cfrac{\\left(y - \\hat{y} \\right)^{2}}{y^2}$ should be minimized, which means that we should find the derivative of loss function w.r.t $\\hat{y}$ and set it to 0.\n",
    "\n",
    "$\\frac{d\\mathcal{L}}{d\\hat{y}} = \\frac{2 (y - \\hat{y})}{y^2} = 0$\\\n",
    "$2(y - \\hat{y}) = 0$\\\n",
    "$y - \\hat{y} = 0$\\\n",
    "$y = \\hat{y}$\n",
    "\n",
    "So, the optimal prediction in the leaf is $\\hat{y} = \\frac{1}{n} \\sum_{i = 1}^n y_i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de7457eba5e06e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T23:25:51.165295Z",
     "start_time": "2023-11-21T23:25:51.146918Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
